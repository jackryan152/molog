#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#       untitled.py
#       
#       Copyright 2011 Jelle <jelle@indigo>
#       
#       This program is free software; you can redistribute it and/or modify
#       it under the terms of the GNU General Public License as published by
#       the Free Software Foundation; either version 3 of the License, or
#       (at your option) any later version.
#       
#       This program is distributed in the hope that it will be useful,
#       but WITHOUT ANY WARRANTY; without even the implied warranty of
#       MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#       GNU General Public License for more details.
#       
#       You should have received a copy of the GNU General Public License
#       along with this program; if not, write to the Free Software
#       Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
#       MA 02110-1301, USA.
#       
#       

import logging
import daemon
import os
import sys
import pymongo
import re
import pika
import json
import pyes
import threading
import cherrypy
import time
from datetime import date
from logging.handlers import SysLogHandler
from pymongo.objectid import ObjectId
from pika.adapters import SelectConnection
from optparse import OptionParser
from multiprocessing import Process, Manager, Queue, Lock
from configobj import ConfigObj
#TODO(smetj): move classes into dedicated files & import.

__version__='0.2.1'

class Logger():
	'''Creates a logger class and writes to screen or syslog.'''
	def __init__(self, loglevel='DEBUG'):
		self.loglevel=loglevel
		self.screen_format=logging.Formatter('%(asctime)s %(levelname)s::%(processName)s:%(message)s')
		self.syslog_format=logging.Formatter('NetCrawl %(processName)s %(message)s')
	def get(self,name=None, scrlog=True, txtlog=True):
		'''Returns a logger object to use.'''
		log = logging.getLogger(name)
		log.setLevel(self.loglevel)
		syslog=SysLogHandler(address='/dev/log')
		syslog.setFormatter(self.syslog_format)
		log.addHandler(syslog)	

		if scrlog == True:
			scr_handler = logging.StreamHandler()
			scr_handler.setFormatter(self.screen_format)
			log.addHandler(scr_handler)
		return log
class MatchWorker(Process):
	'''Matchworker is a worker process which consumes the queue.  It can run in parallel.'''
	def __init__(self,broker_config,elastic_search,mongo_data_api,priority_map,logger,block):
		Process.__init__(self)
		self.broker_config=broker_config
		self.elastic_search=elastic_search
		self.mongo_db=mongo_data_api.db
		self.generateHostStatus=mongo_data_api.generateHostStatus
		self.priority_map=priority_map
		self.logger=logger
		self.block=block
		self.daemon=True
		self.start()
	def run(self):
		self.logger.info('Started')
		#Setup Broker connection
		credentials = pika.PlainCredentials(self.broker_config['username'],self.broker_config['password'])
		self.parameters = pika.ConnectionParameters(self.broker_config['host'],credentials=credentials)
		self.connection = SelectConnection(self.parameters,self.__on_connected)	
		try:
			self.connection.ioloop.start()
		except KeyboardInterrupt:
			self.connection.close()
			self.connection.ioloop.start()
	def __on_connected(self,connection):
		self.logger.info('Connecting to broker.')
		connection.channel(self.__on_channel_open)
	def __on_channel_open(self,new_channel):
		self.channel = new_channel
		self.__initialize()
		self.channel.basic_consume(self.processData, queue = self.broker_config['input_queue'])
		self.channel.basic_qos(prefetch_count=1)
	def __initialize(self):
		self.logger.debug('Creating exchanges, queues and bindings on broker.')
		
		self.channel.exchange_declare(exchange=self.broker_config['input_exchange'],type=self.broker_config['input_exchange_type'],durable=True)
		self.channel.exchange_declare(exchange=self.broker_config['output_exchange'],type=self.broker_config['output_exchange_type'],durable=True)
		
		self.channel.queue_declare(queue=self.broker_config['input_queue'],durable=True)
		self.channel.queue_declare(queue=self.broker_config['output_queue'],durable=True)
		
		self.channel.queue_bind(exchange=self.broker_config['input_exchange'],queue=self.broker_config['input_queue'])
		self.channel.queue_bind(exchange=self.broker_config['output_exchange'],queue=self.broker_config['output_queue'])
	def processData(self,ch, method, properties, body):
		'''Broker connection callback which does all the heavy lifting on the incoming data.'''
		try:
			data = json.loads(body)
		except Exception as err:
			self.logger.warn('Garbage reveived from broker, purging. Reason: %s'%(err))
			self.channel.basic_ack(delivery_tag=method.delivery_tag)
		else:
			if self.ignoreLog(data) == False:
				try:
					feedback=self.writeElastic(data)
					data['@source_host']
					data['@fields']['priority'][0]
					data['@fields']['timestamp8601'][0]
					
					self.writeMongo(	host=data['@source_host'],
								id=feedback['_id'], 
								level=self.priority_map(priority = data['@fields']['priority'][0]),
								date=data['@fields']['timestamp8601'][0]
								)
					self.updateMonitoring(host=data['@source_host'],message=data['@message'])
				except Exception as err:
					self.logger.warn('I could not process record entirely. Resubmitted to queue. Reason: %s'%(err))
				else:
					self.writeElastic(data)
					self.channel.basic_ack(delivery_tag=method.delivery_tag)
			else:
				self.writeElastic(data)
				self.channel.basic_ack(delivery_tag=method.delivery_tag)			
	def ignoreLog(self,document):
		'''Applies all defined regexes on the document to see whether we can ignore it'''
		for regex in self.mongo_db.regexes.find({}).sort('order'):
			if re.match(regex['type'],document['@type']):
				if len(regex['regexes']) > 0:
					flag=True
					for field in regex['regexes']:
						if document['@fields'].has_key(field):
							for data in document['@fields'][field]:
								if not re.match(regex['regexes'][field],data):
									flag = False
			if flag == True:
				return True
		return False
	def writeElastic(self,data):
		'''Writes the incoming data to Elasticsearch.'''
		return self.elastic_search.insert(data)
	def writeMongo(self,host=None,id=None,level=None,date=None):
		'''Writes the references towards the Elasticsearch object in the DB'''
		self.mongo_db.results.insert({'host':host,'level':level,'es_ref':id,'date':date})
	def updateMonitoring(self,host=None,message=None):
		'''Sends an update message to monitoring indicating the total warnings/criticals of a host have changed.'''
		self.logger.debug('Sending update to Nagios')
		message = self.generateHostStatus(host=host)
		self.channel.basic_publish(	exchange=self.broker_config['output_queue'], 
						routing_key='', 
						body=message, 
						properties=pika.BasicProperties(delivery_mode=2)
						)
class SimpleProducer():
	'''Sends Nagios updates to the RabbitMQ broker whenever results of a host change.'''
	def __init__(self,broker_config,logger):
		self.broker_config=broker_config
		credentials = pika.PlainCredentials(self.broker_config['username'],self.broker_config['password'])
		self.parameters = pika.ConnectionParameters(self.broker_config['host'],credentials=credentials)
		connection = pika.BlockingConnection(self.parameters)
		self.channel = connection.channel()
		self.logger = logger
		self.lock = Lock()
	def publish(self,data=None):
		self.lock.acquire()
		self.logger.debug("Sending %s to monitoring."%(data))
		self.channel.basic_publish(	exchange=self.broker_config['output_queue'],
						routing_key='',
						body=data,
						properties=pika.BasicProperties(delivery_mode=2)
						)
		self.lock.release()
class MongoDataAPI():
	'''Creates a MongoDB connection object.'''
	def __init__(self,host,logger,nagios_config=None,rabbitmq_config=None,es_config=None):
		self.connection=pymongo.Connection(host)
		self.db = self.connection.molog
		
		#MessageBuilder constructs messages we want to send back to Nagios.
		self.msg_builder=MessageBuilder(config = nagios_config)
		
		#SimpleProducer sends MessageBuilder messages to the broker infrastructure.
		self.update_monitoring = SimpleProducer(	broker_config=rabbitmq_config,
								logger = logger
							)
		#ElasticSearch handles ElasticSearch IO
		self.es_lookup=ElasticSearch(host=es_config['host'])
		self.logger=logger
	def __checkIntegrity(self):
		pass
	def queryRegex(self,id=None,tags=None):
		query={}
		if id == None:
			query = self.__buildTagQuery(tags=tags)
			list=[]
			for regex in self.db.regexes.find(query).sort('order'):
				regex = self.__replaceID(regex)
				list.append(regex)				
			return json.dumps(list)
		else:
			return json.dumps([self.__replaceID(data=self.db.regexes.find_one( { '_id' : ObjectId(id) } ))])
	def removeRegex(self,id=None,tags=None):
		if id == None:
			query = self.__buildTagQuery(tags=tags)
			try:
				self.db.regexes.remove( query )
			except:
				pass
			else:
				return json.dumps({'status':'ok'})
		else:
			try:
				self.db.regexes.remove( { '_id' : ObjectId(id) } )
			except:
				pass
			else:
				return json.dumps({'status':'ok'})
	def insertRegex(self,document=None):
		if self.__checkRegexIntegrity(document=document) == True:
			self.db.regexes.insert(json.loads(document))
			return json.dumps({"status":"ok"})
		else:
			print document
			return json.dumps({"status":"nok","message":"document not valid"})
	def updateRegex(self,id=None,document=None):
		if self.__checkRegexIntegrity(document=document,partial=True) == True:
			self.db.regexes.update({'_id':ObjectId(id)},{'$set':json.loads(document)})
			return json.dumps({"status":"ok"})
		else:
			return json.dumps({"status":"nok","message":"document not valid"})
	def queryRecord(self,id=None,hostname=None,level=None,limit=0,sort=-1):
		if id == None:
			query = self.__buildHostQuery(hostname=hostname,level=level)
			list=[]
			for result in self.db.results.find(query).limit(limit).sort('date',sort):
				result = self.__replaceID(result)
				message = self.es_lookup.messageMap(id = result['es_ref'] )
				result.update( {'message':message } )
				list.append(result)
			return json.dumps(list)
		else:
			result = self.db.results.find_one({'_id':ObjectId(id)})
			result = self.__replaceID(result)
			message = self.es_lookup.messageMap(id = result['es_ref'] )
			result.update( {'message':message } )
			return json.dumps([result])			
	def deleteRecord(self,id=None,host=None):
		if id != None:
			host_name = self.db.results.find_one({'_id':ObjectId(id)})['host']
			self.db.results.remove({'_id':ObjectId(id)})
		if host != None:
			host_name=host
			self.db.results.remove({'host':host})		
		if self.update_monitoring != False:
			message = self.generateHostStatus(host=host_name)
			self.update_monitoring.publish(data=message)
		return json.dumps({'status':'ok'})
	def queryTotal(self,host=None):
		if host ==None:
			list=[]
			for host in sorted(self.db.results.distinct("host")):
				list.append({	'host':host,
						'warning':self.db.results.find({'host':host,'level':'warning'}).count(),
						'critical':self.db.results.find({'host':host,'level':'critical'}).count()
						})
			return json.dumps(list)
		else:
			return json.dumps([{	'host':host,
						'warning':self.db.results.find({'host':host,'level':'warning'}).count(),
						'critical':self.db.results.find({'host':host,'level':'critical'}).count()
						}])
	def generateHostStatus(self,host=None):
		warnings = self.db.results.find({'host':host,'level':'warning'}).count()
		criticals = self.db.results.find({'host':host,'level':'critical'}).count()
		return self.msg_builder.nagiosService(host=host,warning=warnings,critical=criticals)	
	def __replaceID(self,data=None):
		if data == None or data == '':
			return []
		else:
			data['id'] = str(ObjectId(data['_id']))
			del data['_id']
			return data
	def __convertList(self,list=None):
		if list == None:
			return []
		else:
			return list.split(',')		
	def __buildTagQuery(self,tags=None):
		if tags == None:
			return {}
		else:
			tags = self.__convertList(list=tags)
			return {'tags':{ '$all': tags }}
	def __checkRegexIntegrity(self,document={},partial=False):
		white_list = { 'regexes': dict, 'type':unicode , 'tags':list, 'order':unicode }
		try:
			document = json.loads(document)
			for element in document.keys():
				if element not in white_list.keys():
					raise Exception("%s invalid data in Regex document."%element)
				if type(document[element]) is not white_list[element]:
					raise Exception("%s is invalid type."%element)			
			if partial == False:
				for key in white_list:
					document[key]
			return True
		except Exception as err:
			print err
			self.logger.warning ("Invalid Regex document. Reason: %s"%err)
			return False
	def __buildHostQuery(self,hostname=None,level=None):
		query = {}
		if hostname != None:
			query.update({'host':hostname})
		if level != None:
			query.update({'level':level})
		return query		
class ElasticSearch():
	'''Creates an ElasticSearch connection object'''
	def __init__(self,host):
		self.conn = pyes.ES([host])
	def insert(self,data):
		'''Inserts a document in ES with the right index'''
		return self.conn.index(data,"logstash-%s.%s.%s"%(date.today().year, date.today().month, date.today().day),data['@type'])
	def messageMap(self,id):
		q = pyes.query.IdsQuery(None,id)
		result= self.conn.search(query=q)['hits']['hits'][0]['_source']['@fields']
		return ( result['SYSLOGPROG'][0]+': '+result['message'][0] )		
class WebServerErrors():
	def __init__(self):
		pass
	def E404(self,status, message, traceback, version):
		return json.dumps({ "status":"nok", "error":status, "message":message })
	def E500(self,status, message, traceback, version):
		return json.dumps({ "status":"nok", "error":status, "message":message+" Please submit bugreport to https://github.com/smetj/molog/issues" })
class WebServer(threading.Thread):
	def __init__(self,host,port,ssl="off",ssl_certificate=None,ssl_private_key=None,rest_functions=None,enable_logging=True,logger=None,blockcallback=None):
		threading.Thread.__init__(self)
		self.host=host
		self.port=port
		self.ssl=ssl
		self.ssl_certificate=ssl_certificate
		self.ssl_private_key=ssl_private_key
		self.rest_functions=rest_functions
		self.enable_logging=enable_logging
		self.logger=logger
		self.loop=blockcallback
		self.errors=WebServerErrors()
		self.daemon=True
		self.start()		
	def run(self):
		self.logger.info("WebServer thread started on %s:%s"%(self.host,self.port))
		config={'global': {'server.socket_host': self.host},
			'/': {'request.dispatch': cherrypy.dispatch.MethodDispatcher()}
			}
		cherrypy.config.update(	
								{ 
								'global': 
									{
									'server.socket_port': int(self.port),
									'server.socket_host': self.host,
									'tools.sessions.on': False,
									'log.screen': False
									}
								}
								)
		cherrypy.config.update({'error_page.404': self.errors.E404})
		cherrypy.config.update({'error_page.500': self.errors.E500})		
		#check if we need to run over https or not
		if self.ssl == "on":
			cherrypy.config.update(
									{
									'server.ssl_certificate': self.ssl_certificate,
									'server.ssl_private_key': self.ssl_private_key
									}
								  )	
		
		cp_app = cherrypy.tree.mount(self.rest_functions,'/',config=config)
		
		if self.enable_logging == True:
			cp_app.log.access_log_format = '%(h)s %(l)s %(u)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s"'
			cp_app.log.access_log = self.logger
			cp_app.log.error_log = self.logger
		cherrypy.engine.start()
		while self.loop() == True:
			time.sleep(0.1)
		cherrypy.engine.exit()
		self.logger.info("WebServer thread stopped.")
class RestRegex():
	exposed=True
	def __init__(self,mongo_data_api=None):
		self.mongo_data_api=mongo_data_api
	def GET(self,*args,**kwargs):
		if len(args) == 0:
			return self.mongo_data_api.queryRegex(tags=kwargs.get('tags',None))
		else:
			return self.mongo_data_api.queryRegex(id=args[0])
	def DELETE(self,*args,**kwargs):
		if len(args) == 0:
			return self.mongo_data_api.removeRegex(tags=kwargs.get('tags',None))
		else:
			return self.mongo_data_api.removeRegex(id=args[0])
	def POST(self,*args,**kwargs):
		if len(args) == 0:
			#insert a regex
			return self.mongo_data_api.insertRegex(document = cherrypy.request.body.read() )
		else:
			#update an existing regex
			return self.mongo_data_api.updateRegex(id = args[0], document = cherrypy.request.body.read() )		
class RestRecord():
	exposed=True
	def __init__(self,mongo_data_api=None):
		self.mongo_data_api=mongo_data_api
	def GET(self,*args,**kwargs):
		if len(args) == 0:
			return self.mongo_data_api.queryRecord(hostname=kwargs.get('host',None),level=kwargs.get('level',None),limit=int(kwargs.get('limit',0)),sort=int(kwargs.get('sort',-1)))
		else:
			try:
				return self.mongo_data_api.queryRecord(id=args[0])
			except Exception as err:
				raise cherrypy.HTTPError("404 Not Found", str(err))
				
	def DELETE(self,*args,**kwargs):
		if len(args) == 1:
			return self.mongo_data_api.deleteRecord(id=args[0])
		else:
			return self.mongo_data_api.deleteRecord(host=kwargs.get('host',None))
class RestTotals():
	exposed=True	
	def __init__(self,mongo_data_api=None):
		self.mongo_data_api=mongo_data_api
	def GET(self,*args,**kwargs):
		if len(args) == 0:
			return self.mongo_data_api.queryTotal()
		else:			
			return self.mongo_data_api.queryTotal(host=args[0])
class V1():
	exposed=True
	'''Dummy class to version REST api'''
	def __init__(self):
		help = open('/opt/molog/var/v1_help.html','r')
		self.content = ''.join(help.readlines())
	def GET(self):
		return str(self.content)
class RestFunctions():
	exposed=True	
	def __init__(self,mongo_data_api=None):
		self.mongo_data_api=mongo_data_api
		#Future versions of REST apis can go here?
		self.v1=V1()
		self.v1.regex = RestRegex(mongo_data_api=self.mongo_data_api)
		self.v1.record = RestRecord(mongo_data_api=self.mongo_data_api)
		self.v1.totals = RestTotals(mongo_data_api=self.mongo_data_api)
	def GET(self):
		return "<a href='/v1'>API Version 1</a>"
class MessageBuilder():
	'''Generates a message compliant with the monitoring system.'''
	def __init__(self,config):
		self.config=config
	def nagiosService(self,host,warning,critical):
		url = self.config['urlize']%(host)
		status_txt='OK'
		status_num='0'
		if warning > 0:
			status_txt='Warning'
			status_num='1'
		if critical > 0:
			status_txt='Critical'
			status_num='2'
		return "[%s] PROCESS_SERVICE_CHECK_RESULT;%s;%s;%s;<A HREF='%s'>%s: Host has %s warning and %s critical log entries.</A>"%(time.time(),host,self.config['service'],status_num,url,status_txt,warning,critical)
class Server():
	'''Server class handling process control, startup & shutdown'''
	def __init__(self,config):
		self.cfgfile=config
		self.config=None
		self.block=True
	def __loadConfig(self,config=None):
		try:
			return ConfigObj(config)
		except Exception as err:
			sys.stderr.write('There appears to be an error in your configfile:\n')
			sys.stderr.write('\t'+ str(type(err))+" "+str(err) + "\n" )
			os.kill(os.getpid(),signal.SIGKILL)
	def lock(self):
		return self.block
	def doPID(self):
		if self.checkPIDRunning() == False:
			self.writePID()
	def checkPIDRunning(self):
		'''Checks whether the pid file exists and if it does, checks whether a process is running with that pid.
		Returns False when no process is running with that pid otherwise True'''
		if os.path.isfile(self.config['application']['pid']):
			try:
				pid_file = open(self.config['application']['pid'], 'r')
				pid=pid_file.readline()
				pid_file.close()
			except Exception as err:
				sys.stderr.write('I could not open the pid file. Reason: %s\n'%(err))
				sys.exit(1)
		try:
			os.kill(int(pid),0)
		except:
			return False
		else:
			sys.stderr.write('There is already a process running with pid %s\n'%(pid))
			sys.exit(1)				
	def writePID(self):
		try:
			pid = open ( self.config['application']['pid'], 'w' )
			pid.write (str(os.getpid()))
			pid.close()
		except Exception as err:
			sys.stderr.write('I could not write the pid file. Reason: %s\n'%(err))
			sys.exit(1)
	def deletePID(self):
		try:
			os.remove ( self.config['application']['pid'] )
		except:
			pass				
	def start(self):
		#Creating logging object
		logger = Logger()
		self.logger=logger.get(name=self.__class__.__name__)
		self.logger.warning('started')

		#Load config
		self.config = self.__loadConfig(config=self.cfgfile)

		#Write PID
		self.doPID()
						
		#Create ElasticSearch object
		
		
		#Create Workers
		workers=[]
		for worker in range (int(self.config['application']['workers'])):
			#Create worker object 
			workers.append(	MatchWorker(	broker_config=self.config['rabbitmq'],
							mongo_data_api=MongoDataAPI(	host = self.config['mongodb']['host'],
											es_config = self.config['elasticsearch'],
											nagios_config=self.config['nagios'],
											rabbitmq_config=self.config['rabbitmq'],										
											logger=self.logger),
							elastic_search=ElasticSearch(host=self.config['elasticsearch']['host']),
							priority_map=self.__priorityMap,
							logger=self.logger,
							block=self.lock
							)
					)	
		
		rest_functions = RestFunctions	(	mongo_data_api = MongoDataAPI(	host = self.config['mongodb']['host'],
											es_config = self.config['elasticsearch'],
											nagios_config=self.config['nagios'],
											rabbitmq_config=self.config['rabbitmq'],
											logger=self.logger)
							)
		
		#Start Webserver
		webserver = WebServer	(	host=self.config['API']['host'],
						port=self.config['API']['port'],
						ssl="off",
						ssl_certificate=None,
						ssl_private_key=None,
						rest_functions=rest_functions,
						enable_logging=True,
						logger=self.logger,
						blockcallback=self.lock
						)
						
		while self.lock()==True:
			time.sleep(0.1)
		
		self.logger.info('Exit')
	def stop(self):
		self.block=False
	def __priorityMap(self,priority):
		'''A basic lookup function mapping priority level to either warning or critical.
		It's a strange place for this function so we might want to move it to some sort of tools class.'''
		#TODO(smetj): move this function to a propper class.
		if str(priority) in self.config['priority_map']['critical'].split(','):
			return 'critical'
		elif str(priority) in self.config['priority_map']['warning'].split(','):
			return 'warning'
		else:
			return 'ok'
class Help():
	'''Help function'''
	def __init__(self):
		print ('MoLog %s Copyright 2011 by Jelle Smet <development@smetj.net>' %(__version__))
		print ('''
Description: 

	Molog is a framework which consumes and matches logs from RabbitMQ and sends updates to Nagios and ElasticSearch.

Usage: 

	molog action --config configfile


	Actions:
	
		start		Starts MoLog into the background.
		debug		Starts MoLog into the foregound.
		stop		Stops MoLog.
		
		
	Parameters:
	
		--config	The location of the configuration file.


MoLog is distributed under the Terms of the GNU General Public License Version 3. (http://www.gnu.org/licenses/gpl-3.0.html)

For more information please visit http://www.smetj.net/Molog/
''')


		
if __name__ == '__main__':
	parser = OptionParser()
	parser.add_option("--config", dest="config", default="main.conf", type="string", help="The location of the config file.")	
	(cli_options,cli_actions)=parser.parse_args()	
	try:
		if len(cli_actions ) == 0:
			Help()
			sys.exit(1)
		server=Server(config = cli_options.config)
		if cli_actions[0] == 'start':
			with daemon.DaemonContext():
				server.start()
		elif cli_actions[0] == 'debug':
			server.start()
		else:
			Help()
			sys.exit(1)
	except Exception as err:
		print ("An error has been encountered. Please file a bugreport to https://github.com/smetj/molog/issues containing following information: ")
		print
		print ("\t- The content of your config file")
		print ("\t- This piece of information: %s"%(str(err)))
		print
	except KeyboardInterrupt:
		server.stop()
		server.deletePID()
